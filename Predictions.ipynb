{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_folder = './DATA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomination_file_names = [\n",
    "    'best_picture_academy_awards'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../..')\n",
    "import academy_award_predictor_constants as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_df(nomination_file_name):\n",
    "    filename = \"{0}_{1}{2}\".format(nomination_file_name, c.NOMINATION_FILE_DATA, c.NOMINATION_FILE_PREFIX)\n",
    "    df_data = pd.read_csv(os.path.join(data_folder, filename), index_col=0)\n",
    "    df_data.fillna(0, inplace=True)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_df(nomination_file_name):\n",
    "    filename = \"{0}_{1}{2}\".format(nomination_file_name, c.NOMINATION_FILE_PREDICT, c.NOMINATION_FILE_PREFIX)\n",
    "    df_predict = pd.read_csv(os.path.join(data_folder, filename), index_col=0)\n",
    "    df_predict.fillna(0, inplace=True)\n",
    "    return df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_values(df):\n",
    "    cols = ['Nomination_Film_Title', ]\n",
    "    if 'Nomination_Is_Winner' in df.columns:\n",
    "        cols.append('Nomination_Is_Winner')\n",
    "    return df.drop(columns=cols).values\n",
    "\n",
    "def get_y_values(df):\n",
    "    return df['Nomination_Is_Winner'].values\n",
    "\n",
    "def get_titles(df):\n",
    "    return df['Nomination_Film_Title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_y_distribution(y):\n",
    "    counter = Counter(y)\n",
    "    print(\"win: {0}  \\tloose: {1} \\t{2:.1f}%\".format(counter[1], counter[0], counter[1]/(counter[1] + counter[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_values(nomination_file_name):\n",
    "    df_data = get_data_df(nomination_file_name)\n",
    "    \n",
    "    X = get_x_values(df_data)\n",
    "    Y = get_y_values(df_data)\n",
    "    print(\"Distribution of full data:\")\n",
    "    get_y_distribution(Y)\n",
    "    return X, Y\n",
    "\n",
    "def get_predict_values(nomination_file_name):\n",
    "    df_predict = get_predict_df(nomination_file_name)\n",
    "    \n",
    "    X_predict = get_x_values(df_predict)\n",
    "    X_predict_titles = get_titles(df_predict)\n",
    "    return X_predict, X_predict_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data_split(nomination_file_name):\n",
    "    X, Y = get_data_values(nomination_file_name)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print(\"Distribution of training data\")\n",
    "    get_y_distribution(Y_train)\n",
    "    print(\"Distribution of test data\")\n",
    "    get_y_distribution(Y_test)\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import timeit\n",
    "\n",
    "def predict_for_clf(clf_best, x_row):\n",
    "    prediction = clf_best.predict_proba(x_row.reshape(1, -1))\n",
    "    return prediction[0][1]\n",
    "\n",
    "def gridsearch_for_clf(nomination_file_name, clf, params, name):\n",
    "    print(name)\n",
    "    X_train, X_test, Y_train, Y_test = get_data_split(nomination_file_name)\n",
    "    grid_search = GridSearchCV(clf, params, scoring='roc_auc', n_jobs=1, cv=5)\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"--- %0.3fs seconds ---\" % (timeit.default_timer() - start_time))\n",
    "\n",
    "    print(grid_search.best_params_)\n",
    "    print('The accuracy after grid search is {0:.1f}%'.format(grid_search.best_score_*100))\n",
    "\n",
    "    clf_best = grid_search.best_estimator_\n",
    "\n",
    "    accuracy = clf_best.score(X_test, Y_test)\n",
    "    print('The accuracy on the test set is {0:.1f}%'.format(accuracy*100))\n",
    "    print(\"-----------------------------------------\")\n",
    "    X_predict, X_predict_titles = get_predict_values(nomination_file_name)\n",
    "    print(\"Predicting the nominees\")\n",
    "    results = [{\n",
    "        'prediction': predict_for_clf(clf_best, x_row),\n",
    "        'title': X_predict_titles[idx]\n",
    "    } for idx, x_row in enumerate(X_predict)]\n",
    "    for res in sorted(results, key=lambda res: res['prediction'], reverse=True):\n",
    "        print('{0:.1f}%\\t:  {1}'.format(res['prediction'] * 100, res['title']))\n",
    "    print(\"====================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifiers = [{\n",
    "    'name': 'Multinomial Naive Bayes',\n",
    "    'clf': MultinomialNB(),\n",
    "    'params': {\n",
    "        'fit_prior': (True, False),\n",
    "        'alpha': (0.5, 1.0, 2.0, 4.0),\n",
    "    }\n",
    "},{\n",
    "    'name': 'Random Forest',\n",
    "    'clf': RandomForestClassifier(criterion='gini', n_jobs=-1),\n",
    "    'params': {\n",
    "        'n_estimators': (100, 200, 300),\n",
    "        'min_samples_split': (20, 30, 40),\n",
    "        'max_features': ('sqrt', 'log2', None)\n",
    "    }\n",
    "}]\n",
    "\n",
    "def gridsearch_for_file(nomination_file_name):\n",
    "    for classifier in classifiers:\n",
    "        gridsearch_for_clf(nomination_file_name, classifier['clf'], classifier['params'], classifier['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes\n",
      "Distribution of full data:\n",
      "win: 92  \tloose: 465 \t16.5%\n",
      "Distribution of training data\n",
      "win: 74  \tloose: 371 \t16.6%\n",
      "Distribution of test data\n",
      "win: 18  \tloose: 94 \t16.1%\n",
      "--- 0.141s seconds ---\n",
      "{'alpha': 0.5, 'fit_prior': True}\n",
      "The accuracy after grid search is 64.8%\n",
      "The accuracy on the test set is 64.3%\n",
      "-----------------------------------------\n",
      "Predicting the nominees"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rik\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%\t:  Gisaengchung  \n",
      "95.5%\t:  Marriage Story  \n",
      "6.6%\t:  Ford v Ferrari  \n",
      "0.7%\t:  Jojo Rabbit  \n",
      "0.1%\t:  1917  \n",
      "0.0%\t:  Joker  \n",
      "0.0%\t:  Little Women  \n",
      "0.0%\t:  Once Upon a Time ...in Hollywood  \n",
      "0.0%\t:  The Irishman  \n",
      "====================================================================\n",
      "Random Forest\n",
      "Distribution of full data:\n",
      "win: 92  \tloose: 465 \t16.5%\n",
      "Distribution of training data\n",
      "win: 74  \tloose: 371 \t16.6%\n",
      "Distribution of test data\n",
      "win: 18  \tloose: 94 \t16.1%\n",
      "--- 42.959s seconds ---\n",
      "{'max_features': 'sqrt', 'min_samples_split': 30, 'n_estimators': 100}\n",
      "The accuracy after grid search is 80.3%\n",
      "The accuracy on the test set is 83.9%\n",
      "-----------------------------------------\n",
      "Predicting the nominees\n",
      "37.0%\t:  Joker  \n",
      "36.9%\t:  The Irishman  \n",
      "30.7%\t:  1917  \n",
      "30.6%\t:  Once Upon a Time ...in Hollywood  \n",
      "16.0%\t:  Gisaengchung  \n",
      "15.6%\t:  Marriage Story  \n",
      "12.0%\t:  Little Women  \n",
      "10.8%\t:  Ford v Ferrari  \n",
      "6.1%\t:  Jojo Rabbit  \n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "for nomination_file_name in nomination_file_names:\n",
    "    gridsearch_for_file(nomination_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
